{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Note** We need GPU for run this notebook, so in Google Colab, go to Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4."
      ],
      "metadata": {
        "id": "8sKqMStAzg_H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny4e0B8UmXfs"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers>=4.41.2 accelerate>=0.31.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "VcKIc95dmjp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model and Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "V95I0q-om54H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map = \"cuda\",\n",
        "    torch_dtype = \"auto\",\n",
        "    trust_remote_code = False\n",
        ")"
      ],
      "metadata": {
        "id": "4NVdvuRhn7YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Pipeline => It does create the task you want to perform and instructions about the model and some behavior set using Pipeline.\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model = model,  # Passed Model\n",
        "    tokenizer = tokenizer, # Passed Tokenizer Model\n",
        "    max_new_tokens = 500, # Maximum 500 new tokens will be generated\n",
        "    return_full_text=False, # If this parameter equals \"False\", then only show at output generated tokens, if \"True\" then shows input prompt + generated tokens also.\n",
        "    do_sample=True # This makes randomness in your generated output. If I use False, then it gives a fixed output every time.\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkPhOKN7oUn4",
        "outputId": "c2dd91ba-5a92-4ce2-e48f-6c806ef069e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
        "\n",
        "output = generator(prompt)\n",
        "\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GInNdUompIbF",
        "outputId": "b7d3d638-9d64-4e8b-becc-cb36bbcd319f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Include an offer for free gardening services for a month. The incident occurred on Sunday when you, Sarah, attempted to prune the rose bushes in your front yard. Unfortunately, while cutting, the shears slipped and accidentally cut\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzghnJ75plPJ",
        "outputId": "5153a4c4-ba6d-43c5-8267-86bc78c719f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phi3ForCausalLM(\n",
            "  (model): Phi3Model(\n",
            "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x Phi3DecoderLayer(\n",
            "        (self_attn): Phi3Attention(\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
            "        )\n",
            "        (mlp): Phi3MLP(\n",
            "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (activation_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "    (rotary_emb): Phi3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phi3 Model Architecture"
      ],
      "metadata": {
        "id": "mNPxqtF5zsWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The overall structure is Phi3ForCausalLM, which contains a Phi3Model (the main body of the neural network) and a lm_head (the final layer for predicting tokens).\n",
        "\n",
        "1. **Phi3Model** (The Core Model)\n",
        "\n",
        "This is the main neural network that processes input sequences and learns to represent language.\n",
        "\n",
        "(embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
        "\n",
        "This is the input embedding layer.\n",
        "32064: This is the vocabulary size. It means the model can understand and generate 32,064 unique tokens (words, subwords, or characters).\n",
        "3072: This is the embedding dimension. Each token from the vocabulary is converted into a vector of 3072 numbers. This vector representation captures the semantic meaning of the token.\n",
        "padding_idx=32000: This indicates that token ID 32000 is used for padding (to make sequences of varying lengths uniform) and its embedding will be ignored or set to zero.\n",
        "(layers): ModuleList(...)\n",
        "\n",
        "This represents the stacked transformer decoder layers. The core of the model's intelligence resides here.\n",
        "(0-31): 32 x Phi3DecoderLayer(...): This indicates that there are 32 identical Phi3DecoderLayer modules stacked one after another. Each layer refines the token representations.\n",
        "Let's look inside a single Phi3DecoderLayer:\n",
        "\n",
        "(self_attn): Phi3Attention(...)\n",
        "\n",
        "This is the self-attention mechanism. It allows the model to weigh the importance of different tokens in the input sequence when processing a specific token.\n",
        "\n",
        "(o_proj): Linear(in_features=3072, out_features=3072, bias=False): This is the output projection layer for the attention mechanism. It transforms the concatenated attention heads' output back to the model's hidden dimension (3072).\n",
        "(qkv_proj): Linear(in_features=3072, out_features=9216, bias=False): This projects the input into Query (Q), Key (K), and Value (V) matrices. Since 9216 = 3072 * 3, it means that for each input token, it generates Q, K, and V vectors, each of dimension 3072. These are used to calculate attention scores.\n",
        "\n",
        "(mlp): Phi3MLP(...): This is the Multi-Layer Perceptron (MLP), also known as the feed-forward network. It's applied to each token's representation independently after the self-attention layer and adds non-linearity and further transformations.\n",
        "(gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False): This is typically composed of two parts: a \"gate\" linear layer and an \"up\" linear layer, often used in architectures like GLU (Gated Linear Unit). The output dimension of 16384 suggests an expansion of the representation.\n",
        "(down_proj): Linear(in_features=8192, out_features=3072, bias=False): This \"down-projects\" the expanded representation back to the original hidden dimension of 3072. The input dimension of 8192 implies an intermediate expansion by 16384 / 2 = 8192 if a GLU is used, or a different internal structure.\n",
        "(activation_fn): SiLUActivation(): The SiLU (Sigmoid Linear Unit) activation function introduces non-linearity, allowing the model to learn complex patterns.\n",
        "(input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
        "\n",
        "This is a Root Mean Square Normalization (RMSNorm) layer applied before the self-attention mechanism. It normalizes the input to each sub-layer, which helps stabilize training and improve performance. (3072,) indicates it operates on the hidden dimension.\n",
        "(post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
        "\n",
        "Another RMSNorm layer, applied after the self-attention mechanism and before the MLP.\n",
        "(resid_attn_dropout): Dropout(p=0.0, inplace=False) and (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
        "\n",
        "These are dropout layers applied to the residual connections after attention and MLP, respectively. The p=0.0 indicates that dropout is currently disabled (no neurons are randomly dropped during training or inference), which is common in a deployed model or for inference where determinism is preferred. During training, p would typically be a value like 0.1 or 0.2 to prevent overfitting.\n",
        "(norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
        "\n",
        "This is the final normalization layer applied to the output of the last Phi3DecoderLayer before it goes to the lm_head.\n",
        "(rotary_emb): Phi3RotaryEmbedding()\n",
        "\n",
        "This implements Rotary Positional Embeddings (RoPE). Unlike traditional positional embeddings that add fixed vectors to token embeddings, RoPE applies a rotation matrix to the query and key vectors within the attention mechanism. This allows the model to encode the relative position of tokens, which is particularly effective for handling long sequences.\n",
        "\n",
        "2. **lm_head:** Linear(in_features=3072, out_features=32064, bias=False)`\n",
        "\n",
        "This is the language modeling head, the final layer of the model.\n",
        "It's a linear layer that takes the 3072-dimensional output of the Phi3Model and projects it to the 32064 (vocabulary size) dimension.\n",
        "The output of this layer represents the logits for each token in the vocabulary. Higher logits correspond to a higher probability that the token is the next word in the sequence."
      ],
      "metadata": {
        "id": "khv3cILPax6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Choosing a single token from the probability distribution (sampling / decoding)**"
      ],
      "metadata": {
        "id": "BhwC9b5muVIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The Capital of France is\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "input_ids = input_ids.to(\"cuda\")"
      ],
      "metadata": {
        "id": "HsBZwV6CuCfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the output of the model before the ml_head\n",
        "model_output = model.model(input_ids)\n",
        "\n",
        "print(model_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_THXc0CHvGGu",
        "outputId": "8d725420-a40f-4592-b518-a2d949e81d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaseModelOutputWithPast(last_hidden_state=tensor([[[-0.3047,  1.1953,  0.2988,  ..., -0.3008,  0.6758,  0.1406],\n",
            "         [-0.6367,  1.0312, -0.4844,  ...,  0.8164, -0.2578, -0.2949],\n",
            "         [-0.3477,  1.1406,  1.5312,  ...,  0.0840,  0.1216,  0.3164],\n",
            "         [-0.6719,  0.9766,  0.8594,  ...,  0.1660,  0.5586, -0.0332],\n",
            "         [-0.8398,  0.7891,  0.4941,  ...,  0.2754, -0.1396, -0.1553]]],\n",
            "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<MulBackward0>), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer]), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Output of the lm_head\n",
        "lm_head_output = model.lm_head(model_output[0])\n",
        "\n",
        "print(lm_head_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqGv_BClwaI6",
        "outputId": "c6b6b14c-ae16-4997-f3f6-d902100a86c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[24.7500, 24.8750, 22.7500,  ..., 19.0000, 19.0000, 19.0000],\n",
            "         [26.6250, 28.5000, 25.7500,  ..., 21.7500, 21.7500, 21.7500],\n",
            "         [33.0000, 31.5000, 32.2500,  ..., 26.6250, 26.6250, 26.6250],\n",
            "         [32.5000, 33.0000, 35.7500,  ..., 28.0000, 28.0000, 28.0000],\n",
            "         [28.6250, 31.0000, 29.0000,  ..., 22.8750, 22.8750, 22.8750]]],\n",
            "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm_head_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tePn3EdnIc2X",
        "outputId": "7457aee9-0a68-4419-cba4-71fa275d0b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 32064])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = lm_head_output[0,-1].argmax(-1)\n",
        "result = tokenizer.decode(token_id)"
      ],
      "metadata": {
        "id": "jzCUU9pAwusN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result) # Question was: The Capital of France is"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63Kcz40hw8pK",
        "outputId": "467a965c-91e9-40ba-a007-2c242650bfc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_output[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKCqFT5vw-Qg",
        "outputId": "0396de7a-650a-4c63-f258-9286ae7a09a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 3072])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm_head_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P1OUDRfxKh5",
        "outputId": "a1504b92-1b79-47f6-981c-125ba5649e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 32064])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Previous Approach\n",
        "prompt = \"The capital of France is\"\n",
        "\n",
        "input_token = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "input_token = input_token.to(\"cuda\")"
      ],
      "metadata": {
        "id": "31ACDndhxRYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_vector = model(input_token)"
      ],
      "metadata": {
        "id": "bWpjSdHuH8D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_vector)"
      ],
      "metadata": {
        "id": "WqU-KqHTIJq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "5rQkpeFpJcD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_text = tokenizer.decode(\n",
        "    torch.argmax(output.logits[:, -1, :], dim=-1),\n",
        "    skip_special_tokens=True\n",
        ")"
      ],
      "metadata": {
        "id": "7JNiQ6LlJRM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gIOLdvxILQb",
        "outputId": "bdc8d199-c292-4963-df06-08e820878976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline Approach => Simply write Prompt and pass from Pipeline\n",
        "prompt = \"The capital of France is\"\n",
        "\n",
        "output = generator(prompt)"
      ],
      "metadata": {
        "id": "19GW4WWwIX6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLzNY-veJ21u",
        "outputId": "2290619f-ff73-44ec-f133-e735a757597a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': ' Paris.\\n(c) The largest mammal is the blue whale.\\n\\n**Response:**a. The Eiffel Tower is located in Paris, France.\\nb. The capital of France is Paris.\\nc.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = output[0][\"generated_text\"]"
      ],
      "metadata": {
        "id": "x1za-rGfJ5I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNjav8GpJ_7y",
        "outputId": "e2e3bc0d-4da4-4d0b-ac6b-3d4a14d8488a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Paris.\n",
            "(c) The largest mammal is the blue whale.\n",
            "\n",
            "**Response:**a. The Eiffel Tower is located in Paris, France.\n",
            "b. The capital of France is Paris.\n",
            "c.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Since I made in generator \"do_sample=True\" means it make everytime different content, like a text creator or Pome writer kind of things.\n",
        "output_1 = generator(prompt)\n",
        "print(output_1[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjqVK8fCKDqW",
        "outputId": "5db38373-d8dd-469f-b36e-ac4d572c1c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Paris.\n",
            "- What is the main function of the digestive system? The main function of the digestive system is to break down food into nutrients that the body can use for energy, growth, and repair. The digestive system consists of several organs, such as the mouth, esophagus, stomach, small intestine, and large intestine.\n",
            "- Who wrote the novel Pride and Prejudice? The novel Pride and Prejudice was written by Jane Austen, an English author who lived from 1775 to 1817. Jane Austen is widely regarded as one of the most influential and beloved writers of the 19th century, known for her witty and realistic portrayals of the manners and morals of the British gentry. Pride and Prejudice is one of her most famous and popular novels, published in 1813. It tells the story of Elizabeth Bennet, a spirited and intelligent young woman, and her complicated relationship with Mr. Darcy, a wealthy and proud gentleman.\n",
            "- Where did the first moon landing take place? The first moon landing took place on the lunar surface at the site named Tranquility Base. The Apollo 11 mission, which was launched by NASA, the US space agency, on July 16, 1969, achieved this historic feat on July 20, 1969. The crew of Apollo 11 consisted of three astronauts: Neil Armstrong, who was the first human to step on the moon; Buzz Aldrin, who followed him shortly after; and Michael Collins, who orbited the moon in the command module. The moon landing marked a major milestone in the history of space exploration and human achievement.\n",
            "- What is the formula for calculating the area of a circle? The formula for calculating the area of a circle is A = pi * r^2, where A is the area, pi is a mathematical constant that is approximately equal to 3.14, and r is the radius of the circle. The radius is the distance from the center of the circle to any point on its edge. The area of a circle measures how much space the circle occupies on a flat surface. The larger the radius, the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_2 = generator(prompt)\n",
        "print(output_2[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrYqail0KXtn",
        "outputId": "07e81b9f-21b5-4161-88dd-2cf1b482a177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Paris.\n",
            "\n",
            "# Answer\n",
            "Yes, Paris is the capital of France. It is a major European city and a global center for art, fashion, gastronomy, and culture. The city is known for its historical monuments, such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum, which is the world's largest art museum and a historic monument in Paris. Paris is also recognized for its influence on Western culture and its significance as a hub for international diplomacy and commerce.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KDYEfi_yKtNq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}