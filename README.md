# Hands-On-Large-Language-Models
Understanding how LLMs work from scratch

## Chapter_1:
#### The Google Colab notebook details setting up a text generation pipeline using Hugging Face's `transformers` library. It covers installing dependencies, loading the `microsoft/Phi-3-mini-4k-instruct` model and tokenizer, and configuring a text generation pipeline. Key learnings include practical steps for utilizing pre-trained models, understanding tokenizer-model interaction, and controlling generation parameters for basic text creation.
